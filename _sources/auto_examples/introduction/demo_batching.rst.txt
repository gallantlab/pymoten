.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_introduction_demo_batching.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_introduction_demo_batching.py:


===========================================
 Extracting features from stimulus batches
===========================================

This example shows how to use batches to extract motion-energy features from a video.

When the stimulus is very high-resolution (e.g. 4K) or is multiple hours long, it might not be possible to fit the data in memory. In such situations, it is useful to load a small number of video frames and extract motion-energy features from that subset of frames alone. In order to do this properly, one must avoid edge effects. In this example we show how to do that.

First, we'll specify the stimulus we want to load.


.. code-block:: default


    import moten
    import numpy as np
    import matplotlib.pyplot as plt
    stimulus_fps = 24
    video_file = 'http://anwarnunez.github.io/downloads/avsnr150s24fps_tiny.mp4'








Load the first 300 images and spatially downsample the video.


.. code-block:: default

    small_vhsize = (72, 128)        # height x width
    luminance_images = moten.io.video2luminance(video_file, size=small_vhsize, nimages=300)
    nimages, vdim, hdim = luminance_images.shape
    print(vdim, hdim)

    fig, ax = plt.subplots()
    ax.matshow(luminance_images[200], vmin=0, vmax=100, cmap='inferno')
    ax.set_xticks([])
    ax.set_yticks([])




.. image:: /auto_examples/introduction/images/sphx_glr_demo_batching_001.png
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    72 128

    []



Next we need to construct the pyramid and extract the motion-energy features from the full stimulus.


.. code-block:: default


    pyramid = moten.pyramids.MotionEnergyPyramid(stimulus_vhsize=(vdim, hdim),
                                                 stimulus_fps=stimulus_fps,
                                                 filter_temporal_width=16)

    moten_features = pyramid.project_stimulus(luminance_images)
    print(moten_features.shape)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (300, 2530)




We have to include some padding to the batches in order to avoid convolution edge effects. The padding is determined by the temporal width of the motion-energy filter. By default, the temporal width is 2/3 of the stimulus frame rate (`int(fps*(2/3))`). This parameter can be specified when instantating a pyramid by passing e.g. ``filter_temporal_width=16``. Once the pyramid is defined, the parameter can also be accessed from the ``pyramid.definition`` dictionary.


.. code-block:: default


    filter_temporal_width = pyramid.definition['filter_temporal_width']








Finally, we define the padding window as half the temporal filter width.


.. code-block:: default


    window = int(np.ceil((filter_temporal_width/2)))
    print(filter_temporal_width, window)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    16 8




Now we are ready to extract motion-energy features in batches:


.. code-block:: default


    nbatches = 5
    batch_size = int(np.ceil(nimages/nbatches))
    batched_data = []
    for bdx in range(nbatches):
        start_frame, end_frame = batch_size*bdx, batch_size*(bdx + 1)
        print('Batch %i/%i [%i:%i]'%(bdx+1, nbatches, start_frame, end_frame))

        # Padding
        batch_start = max(start_frame - window, 0)
        batch_end = end_frame + window
        batched_responses = pyramid.project_stimulus(
            luminance_images[batch_start:batch_end])

        # Trim edges
        if bdx == 0:
            batched_responses = batched_responses[:-window]
        elif bdx + 1 == nbatches:
            batched_responses = batched_responses[window:]
        else:
            batched_responses = batched_responses[window:-window]
        batched_data.append(batched_responses)

    batched_data = np.vstack(batched_data)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Batch 1/5 [0:60]
    Batch 2/5 [60:120]
    Batch 3/5 [120:180]
    Batch 4/5 [180:240]
    Batch 5/5 [240:300]




They are exactly the same.


.. code-block:: default

    assert np.allclose(moten_features, batched_data)








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  27.529 seconds)


.. _sphx_glr_download_auto_examples_introduction_demo_batching.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: demo_batching.py <demo_batching.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: demo_batching.ipynb <demo_batching.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
