{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n#  Extracting features from stimulus batches\n\n\nThis example shows how to use batches to extract motion-energy features from a video.\n\nWhen the stimulus is very high-resolution (e.g. 4K) or is multiple hours long, it might not be possible to fit the data in memory. In such situations, it is useful to load a small number of video frames and extract motion-energy features from that subset of frames alone. In order to do this properly, one must avoid edge effects. In this example we show how to do that.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we'll specify the stimulus we want to load.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import moten\nimport numpy as np\nimport matplotlib.pyplot as plt\nstimulus_fps = 24\nvideo_file = 'http://anwarnunez.github.io/downloads/avsnr150s24fps_tiny.mp4'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the first 300 images and spatially downsample the video.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "small_vhsize = (72, 128)        # height x width\nluminance_images = moten.io.video2luminance(video_file, size=small_vhsize, nimages=300)\nnimages, vdim, hdim = luminance_images.shape\nprint(vdim, hdim)\n\nfig, ax = plt.subplots()\nax.matshow(luminance_images[200], vmin=0, vmax=100, cmap='inferno')\nax.set_xticks([])\nax.set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we need to construct the pyramid and extract the motion-energy features from the full stimulus.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pyramid = moten.pyramids.MotionEnergyPyramid(stimulus_vhsize=(vdim, hdim),\n                                             stimulus_fps=stimulus_fps,\n                                             filter_temporal_width=16)\n\nmoten_features = pyramid.project_stimulus(luminance_images)\nprint(moten_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have to include some padding to the batches in order to avoid convolution edge effects. The padding is determined by the temporal width of the motion-energy filter. By default, the temporal width is 2/3 of the stimulus frame rate (`int(fps*(2/3))`). This parameter can be specified when instantating a pyramid by passing e.g. ``filter_temporal_width=16``. Once the pyramid is defined, the parameter can also be accessed from the ``pyramid.definition`` dictionary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filter_temporal_width = pyramid.definition['filter_temporal_width']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we define the padding window as half the temporal filter width.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "window = int(np.ceil((filter_temporal_width/2)))\nprint(filter_temporal_width, window)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are ready to extract motion-energy features in batches:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nbatches = 5\nbatch_size = int(np.ceil(nimages/nbatches))\nbatched_data = []\nfor bdx in range(nbatches):\n    start_frame, end_frame = batch_size*bdx, batch_size*(bdx + 1)\n    print('Batch %i/%i [%i:%i]'%(bdx+1, nbatches, start_frame, end_frame))\n\n    # Padding\n    batch_start = max(start_frame - window, 0)\n    batch_end = end_frame + window\n    batched_responses = pyramid.project_stimulus(\n        luminance_images[batch_start:batch_end])\n\n    # Trim edges\n    if bdx == 0:\n        batched_responses = batched_responses[:-window]\n    elif bdx + 1 == nbatches:\n        batched_responses = batched_responses[window:]\n    else:\n        batched_responses = batched_responses[window:-window]\n    batched_data.append(batched_responses)\n\nbatched_data = np.vstack(batched_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They are exactly the same.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert np.allclose(moten_features, batched_data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}